{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "n_train = train.shape[0]\n",
    "n_test = test.shape[0]\n",
    "\n",
    "test_id = test['Id']\n",
    "y = train.SalePrice\n",
    "\n",
    "train = train.drop('Id',axis=1)\n",
    "test = test.drop('Id',axis=1)\n",
    "\n",
    "# normalization the target ,预测结果记得变回来\n",
    "y = np.log1p(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 79)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.concat([train,test])\n",
    "all_data.drop('SalePrice',axis=1,inplace=True)\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_count = all_data.isnull().sum().sort_values(ascending=False)\n",
    "miss_per = (all_data.isnull().sum()/all_data.isnull().count()).sort_values(ascending=False)\n",
    "miss_data = pd.concat([miss_count,miss_per],axis=1,keys=['count','percent'])\n",
    "drop_index = miss_data[miss_data['percent']>0.15].index\n",
    "# miss_index = miss_data[(miss_data['percent']<0.15) & (miss_data['count']>0)].index\n",
    "# drop 丢失率大于15%的数据\n",
    "all_data.drop(drop_index,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling null data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对数字取中位数\n",
    "all_data = all_data.fillna(all_data.mean())\n",
    "\n",
    "miss_count = all_data.isnull().sum().sort_values(ascending=False)\n",
    "miss_index = miss_count[miss_count>0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对cat类取分布最多的值\n",
    "for index in miss_index:\n",
    "    all_data[index] = all_data[index].fillna(all_data[index].value_counts().keys()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_index = all_data.dtypes[train.dtypes == \"object\"].index\n",
    "num_index = all_data.dtypes[train.dtypes != \"object\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# half_num_index = []\n",
    "# count_index = []\n",
    "# for index in num_index:\n",
    "#     count = len(train[index].unique())\n",
    "#     if count < 30:\n",
    "#         half_num_index.append(index)\n",
    "#         count_index.append(count)\n",
    "# half_num_index\n",
    "# count_index\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# for c in half_num_index:\n",
    "#     lbl = LabelEncoder() \n",
    "#     lbl.fit(list(all_data[c].values)) \n",
    "#     train[c] = lbl.transform(list(all_data[c].values))\n",
    "# all_data[half_num_index].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对num进行normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "skew_features = all_data[num_index].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "skew_index = skew_features[skew_features > 0.75].index\n",
    "all_data[skew_index] = np.log1p(all_data[skew_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 73)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对cat类进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 269)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.get_dummies(all_data)\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = all_data[:n_train]\n",
    "test = all_data[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "\n",
    "def rmse_cv(model):\n",
    "    kf = KFold(n_fold,shuffle=True,random_state=0).get_n_splits(train)\n",
    "    rmse = np.sqrt(-cross_val_score(model,train,y,scoring='neg_mean_squared_error',cv=kf))\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(),Lasso(alpha=0.0005,random_state=1))\n",
    "Enet = make_pipeline(RobustScaler(),ElasticNet(alpha=0.0005,l1_ratio=0.9,random_state=1))\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1, nthread = -1)\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r1 = rmse_cv(lasso)\n",
    "r2 = rmse_cv(Enet)\n",
    "r3 = rmse_cv(GBoost)\n",
    "r4 = rmse_cv(model_xgb)\n",
    "r5 = rmse_cv(model_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AverageModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self,models):\n",
    "        self.models = models\n",
    "    def fit(self,X,y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        for model in self.models_:\n",
    "            model.fit(X,y)\n",
    "        return self\n",
    "    def predict(self,X):\n",
    "        predictions = np.column_stack([model.predict(X) for model in self.models_])\n",
    "        return np.mean(predictions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ave_model = AverageModels(models=(lasso,Enet,GBoost))\n",
    "ave_score = rmse_cv(ave_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AverageModels(models=(Pipeline(memory=None,\n",
       "     steps=[('robustscaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
       "       with_scaling=True)), ('lasso', Lasso(alpha=0.0005, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_...           presort='auto', random_state=5, subsample=1.0, verbose=0,\n",
       "             warm_start=False)))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.4603,\n",
       "       gamma=0.0468, learning_rate=0.05, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1.7817, missing=None, n_estimators=2200,\n",
       "       nthread=-1, objective='reg:linear', reg_alpha=0.464,\n",
       "       reg_lambda=0.8571, scale_pos_weight=1, seed=0, silent=1,\n",
       "       subsample=0.5213)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(bagging_fraction=0.8, bagging_freq=5, bagging_seed=9,\n",
       "       boosting_type='gbdt', colsample_bytree=1.0, feature_fraction=0.2319,\n",
       "       feature_fraction_seed=9, learning_rate=0.05, max_bin=55,\n",
       "       max_depth=-1, min_child_samples=10, min_child_weight=5,\n",
       "       min_data_in_leaf=6, min_split_gain=0.0, min_sum_hessian_in_leaf=11,\n",
       "       n_estimators=720, n_jobs=-1, num_leaves=5, objective='regression',\n",
       "       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "       subsample=1.0, subsample_for_bin=50000, subsample_freq=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ave_model.fit(train,y)\n",
    "ave_train_prediction = ave_model.predict(train)\n",
    "ave_rediction = np.expm1(ave_model.predict(test))\n",
    "rms_ave = rmsle(y,ave_train_prediction)\n",
    "\n",
    "model_xgb.fit(train,y)\n",
    "xgb_train_prediction = model_xgb.predict(train)\n",
    "xgb_rediction = np.expm1(model_xgb.predict(test))\n",
    "rms_xgb = rmsle(y,xgb_train_prediction)\n",
    "\n",
    "model_lgb.fit(train,y)\n",
    "lgb_train_prediction = model_lgb.predict(train)\n",
    "lgb_rediction = np.expm1(model_lgb.predict(test))\n",
    "rms_lgb = rmsle(y,lgb_train_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.084514189019064462, 0.080097747429496505, 0.076313672913250877)\n"
     ]
    }
   ],
   "source": [
    "print(rms_ave,rms_xgb,rms_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_prediction = ave_rediction*0.4 + xgb_rediction*0.3 + lgb_rediction*0.3\n",
    "submission = pd.DataFrame({\"Id\":test_id, \"SalePrice\":final_prediction})\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense,Activation\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1022, 269)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 1)                 270       \n",
      "=================================================================\n",
      "Total params: 270\n",
      "Trainable params: 270\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1022 samples, validate on 438 samples\n",
      "Epoch 1/10\n",
      "1022/1022 [==============================] - 0s - loss: 93.7197 - val_loss: 50.8316\n",
      "Epoch 2/10\n",
      "1022/1022 [==============================] - 0s - loss: 23.3373 - val_loss: 18.9361\n",
      "Epoch 3/10\n",
      "1022/1022 [==============================] - 0s - loss: 6.5871 - val_loss: 9.4327\n",
      "Epoch 4/10\n",
      "1022/1022 [==============================] - 0s - loss: 2.0788 - val_loss: 6.3188\n",
      "Epoch 5/10\n",
      "1022/1022 [==============================] - 0s - loss: 0.8337 - val_loss: 4.8160\n",
      "Epoch 6/10\n",
      "1022/1022 [==============================] - 0s - loss: 0.4051 - val_loss: 4.2911\n",
      "Epoch 7/10\n",
      "1022/1022 [==============================] - 0s - loss: 0.2329 - val_loss: 3.8842\n",
      "Epoch 8/10\n",
      "1022/1022 [==============================] - 0s - loss: 0.1549 - val_loss: 3.7489\n",
      "Epoch 9/10\n",
      "1022/1022 [==============================] - 0s - loss: 0.1131 - val_loss: 3.6333\n",
      "Epoch 10/10\n",
      "1022/1022 [==============================] - 0s - loss: 0.0943 - val_loss: 3.5510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5cf8d6ce10>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = StandardScaler().fit_transform(train)\n",
    "scaler = StandardScaler().fit(train)\n",
    "sc_train = scaler.transform(train)\n",
    "x_train,x_test,y_train,y_test = train_test_split(sc_train,y,test_size=0.3,random_state=0)\n",
    "x_train.shape\n",
    "model_keras = Sequential()\n",
    "model_keras.add(Dense(1,input_dim=x_train.shape[1],W_regularizer=l1(0.001)))\n",
    "model_keras.compile(loss='mse',optimizer='sgd')\n",
    "model_keras.summary()\n",
    "model_keras.fit(x_train,y_train,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0526166593058177"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_train_prediction = model_keras.predict(sc_train)\n",
    "rms_keras = rmsle(y,keras_train_prediction)\n",
    "rms_keras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
